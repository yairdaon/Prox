\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{cite}
\usepackage{graphicx}
\usepackage{algorithm} % algorithm package
\usepackage[noend]{algpseudocode}
\usepackage[margin=0.5in]{geometry}
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text





%----------------------------------------------------------------------------------------
% new commands
%----------------------------------------------------------------------------------------
\newcommand{\der}{\text{d}}
\newcommand{\coder}[1]{\texttt{#1}}
\newcommand{\inner}[2]{#1 \cdot #2}
\newcommand{\prox}{\text{Prox}}
\newcommand{\grad}{\nabla_{d} }
\newcommand{\modd}{\text{ mod }}
\newcommand{\divg}{\text{div}}
\title{Proximal Methods for Image Deblurring}

\author{Yair Daon}
\date{}

\pdfinfo{%
  /Title    ()
  /Author   (Yair Daon)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}


\begin{document}
\maketitle
\begin{abstract}
  I describe and implement a method of deblurring images using
  proximal operators.
\end{abstract}

\section{Problem Description}
I follow, somewhat loosely, a deblurring method presented in
\cite{green2015bayesian}. The goal is to recover a high resolution
image $\theta \in \mathbb{R}^{N \times N}$ from a blurred and noisy
observation $y \in \mathbb{R}^{N \times N}$. This obeserved image $y$
is obtained from the original image $\theta$ by a noisy convolution as
follows. Let $H: \mathbb{R}^{N \times N} \to \mathbb{R}^{N \times N}$
be the blurring operator. It acts as $H\theta = \theta * K$, where $K
\in \mathbb{R}^{N \times N}$ is a convolution kernel and the
convolution is taken with periodic boundary conditions.  The blurred
image is further corrupted by gaussian noise, which is $\mathcal{N}(0,
\sigma^2)$, additive and independent for every pixel. Thus $y \sim
\mathcal{N}( H\theta, \sigma^2I)$, where $I: \mathbb{R}^{N \times N}
\to \mathbb{R}^{N \times N}$ is the identity operator.

\section{Hierarchical Model and $\grad$}
This problem is ill posed and needs regularization. The regularization
method comes from the following bayesian hierarchical model suggested
by \cite{oliveira2009adaptive}.

\begin{align*}
  f(y | \theta) &= \frac{1}{ (2\pi \sigma^2)^{\frac{N^2}{2}}} \exp( -\frac{1}{2\sigma^2}\|H\theta - y\|_2^2 )
 \text{ (likelihood) }\\
  \pi( \theta | \alpha ) &= \frac{1}{Z(\alpha)} \exp( -\alpha \|\grad\theta\|_2 ) \text{ (prior) }\\
  \pi(\alpha) &= e^{-\alpha} \mathbf{1}_{\mathbb{R}_{+}}(\alpha) \text{ (hyperprior)}. \\
\end{align*} 

$\grad \theta$ is the discrete gradient and is defined below with a
periodic boundary (slightly differently from the definition in
\cite{chambolle2004algorithm}):
\begin{align*}
  (\grad \theta)_{ij} :&= \big ( (\grad \theta)_{ij}^1 , ( \grad
  \theta)_{ij}^2 \big ) \\
  %
  %
  (\grad \theta)_{ij}^1 :&= \theta_{i+1\modd N,j} -\theta_{ij} \\
  %
  %
  (\grad\theta)_{ij}^2 :&= \theta_{i,j+1\modd N}
  -\theta_{ij}.
  %
  %
\end{align*}

In the discussion below, I drop the $\mod$ term when referring to
$\grad$. In \cite{green2015bayesian}, the norm on the gradient in the
prior is taken to be ``the $l_1-l_2$ composite norm''. Here I use the
2-norm for the gradient, which is the one used in
\cite{chambolle2004algorithm, oliveira2009adaptive}.

\subsection{The posterior}
We seek the MAP estimator of $p( \theta | y )$. First we calculate the
posterior $p( \theta | y )$.  The following discussion follows the one
in \cite[section 4.1]{oliveira2009adaptive} with the simplification
that, in their notation, we take $\alpha = \beta = 1$. First, we find
the marginal $\pi( \theta )$.

\begin{align*}
  \pi (\theta) &= \int_{0}^{\infty}  \pi(\theta| \alpha ) \pi(\alpha) d\alpha\\
  &= \int_{0}^{\infty}  \pi(\theta| \alpha ) e^{-\alpha} d\alpha \\
  &= \int_{0}^{\infty}  \frac{1}{Z(\alpha)} \exp( -\alpha \|\grad\theta\|_2 ) e^{-\alpha} d\alpha \\
\end{align*}

We calculate the normalization constant. Explanations follow.

\begin{align*}
  Z(\alpha) &= \int_{\mathbb{R}^{N \times N}} \exp( -\alpha \|\grad\theta\|_2 ) d\theta  \\
  &= \int_{\mathbb{R}^{N \times N}} \exp( -\alpha \sum_{i,j=1}^{N-1} \sqrt{ (\theta_{i+1,j} -\theta_{ij})^2 + (\theta_{i,j+1} -\theta_{ij})^2} ) d\theta \\
  &\approx \left (\int_{\mathbb{R}^2}  \exp( -\alpha \sqrt{u^2 + v^2} )dudv \right )^{N^2} \\
  &= \left ( 2\pi \int_0^{\infty} \exp(-\alpha r) rdr \right )^{N^2} \\
  &= \left (2\pi \left (-\frac{r}{\alpha}\exp(-\alpha r) \big
  |_{r=0}^{r=\infty} +\frac{1}{\alpha} \int_0^{\infty}\exp(-\alpha
  r) dr \right ) \right)^{N^2}\\
  &= (\frac{2\pi}{\alpha^2})^{N^2} \\
  & \approx C \alpha^{-\eta N^2}.
\end{align*} 

The first approximation builds on an assumption from
\cite{oliveira2009adaptive} that the graditents of different pixels
are independent. Then integral is calculated via integration by
parts. In the last line, $C$ is an irrelevant constant and $\eta =
2$. Since independence does not hold, \cite{oliveira2009adaptive} use
different values of $\eta$ for better performance ($\eta= 0.4$,
specifically) and \cite{green2015bayesian} uses $\eta = 1$ and this is
the value used here. Then:

\begin{align*}
 \pi (\theta) &\approx \frac{1}{C} \int_{0}^{\infty} \alpha^{N^2 } \exp( -\alpha (\|\grad\theta\|_2 + 1)) d\alpha \\
 &= \frac{1}{C} \int_{0}^{\infty} \alpha^{N^2} \exp( -\alpha (\|\grad\theta\|_2 + 1)) d\alpha \\
 %
 %
 &= \frac{1}{C} \left (
 \frac{-\alpha^{N^2} \exp(-\alpha (\|\grad\theta\|_2 + 1)}{||\grad\theta||_2 + 1} \bigg |_{\alpha=0}^{\alpha=\infty}
 +\frac{N^2}{\|\grad\theta\|_2 + 1}\int_{0}^{\infty} \alpha^{N^2-1} \exp( -\alpha (\|\grad\theta\|_2 + 1)) d\alpha \right ) \\
 %
 %
 &= \frac{N^2}{C\|\grad\theta\|_2 + 1}\int_{0}^{\infty} \alpha^{N^2-1} \exp( -\alpha (\|\grad\theta\|_2 + 1)) d\alpha \\
 %
 &= \frac{ N^2!}{ C(\|\grad\theta\|_1 + 1)^{N^2+1}} \text{ (repeated integration by parts)}\\
 &\propto (\|\grad\theta\|_2 + 1)^{-N^2-1},
\end{align*}

which is equation (24) from \cite{oliveira2009adaptive} (with $\alpha
= \beta = 1$ by their notation). Putting the pieces together, the
posterior is:

\begin{align*}
  p(\theta | y ) &\propto f(y | \theta ) \pi(\theta ) \\
  &\propto \exp \left ( -\frac{1}{2\sigma^2} \|H\theta - y\|_2^2 - (N^2 + 1) \log (\|\grad\theta\|_2 + 1) \right ) \\
\end{align*}
 
and so 

\begin{equation*}
\theta_{\text{MAP}} = \argmin_{\theta} \frac{1}{2\sigma^2} \|H\theta - y\|_2^2 + (N^2 + 1) \log (\|\grad\theta\|_2 + 1),
\end{equation*}

which is the maximization problem (21) from \cite{green2015bayesian}
except that we use the 2-norm and not the composite 1,2-norm and our
$N^2$ is denoted there by $n$.

\subsection{Convex Majorants}

The above minimization problem is not convex - for once, log is concave.
This is circumvented in \cite{oliveira2009adaptive} by taking a sequence of convex majorants. 
Consider the problem of finding $\hat{\theta} \in \argmin_{x} L(\theta)$, for some $L$. Carrying out the majorization-minimization 
approach consists of finding a bound $Q(\theta; \theta') \geq L(\theta), \forall \theta,\theta'$
with equality for $\theta=\theta'$
and then iterating $\theta^{(t+1)} := \argmin_{\theta} Q(\theta;\theta^{(t)})$. This iteration is monotone:
\begin{align*}
  L(\theta^{(t+1)}) &= L(\theta^{(t+1)}) - Q(\theta^{(t+1)}; \theta^{(t)}) + Q(\theta^{(t+1)}; \theta^{(t)})\\
  %
  %
  %
  &\leq  Q(\theta^{(t+1)}; \theta^{(t)}) \text{ by } Q \geq L \\
  %
  % 
  %
  & \leq Q(\theta^{(t)} ;\theta^{(t)}) \text{ by definition of } \theta^{(t+1)} \\
  %
  %
  %
  &= L(\theta^{(t)}) \text{ by the equality condition above. }
\end{align*}
Define 

$$
L(\theta) :=\frac{1}{2\sigma^2} \|H\theta - y\|_2^2 + (N^2 + 1) \log (\|\grad\theta\|_2 + 1).
$$

Note that $\forall z,z_0 > 0$:

$$
\log z \leq \log z_0 + \frac{z-z_0}{z_0},
$$ 
with equality iff $z = z_0$.

Use this inequality with $z = \|\grad\theta\|_2 + 1, z_0 = \|\grad
\theta^{(t)}\|_2 + 1$ to observe

\begin{align*}
  \log (\|\grad\theta\|_2 + 1) &\leq \log( \|\grad \theta^{(t)}\|_2 + 1 ) + 
  \frac{ \|\grad \theta\|_2 + 1 - (\|\grad \theta^{(t)}\|_2 + 1)}{\|\grad \theta^{(t)}\|_2 + 1} \\
  &= C(\theta^{(t)}) + \frac{ \|\grad \theta\|_2 }{\|\grad \theta^{(t)}\|_2 + 1}.
\end{align*}

Denote $\alpha^{(t)} := (N^2+1)( \|\grad\theta^{(t)} \|_2 + 1 )^{-1}$. Then

\begin{align*}
  L(\theta) &= \frac{1}{2\sigma^2} \|H\theta - y\|_2^2 + (N^2 + 1) \log (\|\grad\theta\|_2 + 1) \\ 
  &\leq \frac{1}{2\sigma^2} \|H\theta - y\|_2^2 + \alpha^{(t)} \|\grad \theta\|_2   + C(\theta^{(t)})\\
  &=: Q(\theta ; \theta^{(t)}).
\end{align*}

Thus, we find the next approximation by:

\begin{align*}
  \theta^{(t+1)} &:= \argmin_{\theta} Q(\theta, \theta^{(t)}) \\
  &= \argmin_{\theta}  \frac{1}{2\sigma^2} \|H\theta - y\|_2^2 + \alpha^{(t)} \|\grad \theta\|_2
\end{align*}

and $C(\theta^{(t)})$ is omitted since it does not affect the
minimizer. This is the problem that (should be) denoted by (22) in
\cite{green2015bayesian} (one of the authors made a typo and uses a
different $\alpha^{(t)}$. I confirmed this with the author).

Now that we have a convex optimization problem, we'd like to use Douglas Rachford. 
Define
\begin{align*}
  g(\theta) &:=  \frac{1}{2\sigma^2} \|H\theta - y\|_2^2 \\
  h(\theta) &:= \alpha^{(t)} \|\grad \theta\|_2. \\
\end{align*}

We need to find the proximal maps.


\section{Calculating $\prox_h$}
This section follows \cite{chambolle2004algorithm}. Let $J(\theta) := \|\grad \theta \|_2 = \sum_{i,j} |(\grad \theta_{ij})|$.
Note that $J(\lambda \theta) = \lambda J(\theta)$ for $\lambda \geq 0$ and also $J \geq 0$. If $\exists \theta_0$ s.t.
$\langle \phi,\theta_0 \rangle - J(\theta_0) > 0$,
then $\langle \lambda\theta_0, \phi \rangle - J(\lambda \theta_0) \to \infty$ as $\lambda \to \infty$. Thus we may easily conclude,

\begin{align*}
  J^{*}(\phi) :&= \sup_{\theta} \langle \phi, \theta \rangle - J(\theta) \\
  %
  %
  % 
  &= \sup_{\theta} \sum_{i,j = 1}^{N} \phi_{ij} \theta_{ij} - J( \theta ) \\ 
  %
  %
  %
  &= 
  \begin{cases}
    0 & \phi \in K\\
    \infty & \phi \not \in K.\\ 
  \end{cases}
\end{align*}
 
$K$ is convex since $J^{*}$ is. Since $J$ is convex lsc, we observe
that

\begin{align*}
  J(\theta) &= J^{**}(\theta) \\ 
  &= \sup_{\phi} \langle \phi, \theta \rangle - J^{*}(\phi) \\
  %
  %
  %
  &= \sup_{\phi\in K} \langle \phi, \theta \rangle
\end{align*}

By Cauchy Schwarz (and its equality condition)   
\begin{align*}
  J(\theta) &= \sum_{ij} | \grad \theta | \\ 
  &= \sup_{|p_{ij}| \leq 1} \sum_{ij} (\grad\theta)^1_{ij} p^{1}_{ij} + (\grad\theta)^2_{ij} p^{2}_{ij} \\
  %
  %
  %
  &= \sup_{|p_{ij}| \leq 1} \langle \grad\theta, p  \rangle \\
  %
  %
  &= \sup_{|p_{ij}| \leq 1} \langle \theta, \grad^{*} p  \rangle \\
\end{align*}

with the obvious definition of an inner product. If we denote $ \divg
:= -\grad^{*}$, the negative adjoint of the discrete gradient
operator, then we may easily observe $K = \{ \divg p: |p_{ij}| \leq 1
\ \forall 1 \leq i,j, \leq N \}$. I won't write the expression for
$\divg$ here but it is extremely simple because of the periodic
boundary. We may now turn to deriving an algorithm for the proximity
mapping.

\begin{align*}
  p &= \prox_{\lambda J}(\theta) \\ 
  %
  % 
  % 
  &= \argmin_{x} \frac{1}{2}\| x- \theta \|^2 + \lambda J(x) \\
  % 
  % 
  % 
  \Leftrightarrow 0 &\in \frac{p - \theta}{\lambda} + \partial J(p) \\
  % 
  % 
  % 
  \Leftrightarrow \frac{\theta - p}{\lambda} &\in \partial J(p) \\
  % 
  % 
  % 
  \Leftrightarrow p &\in \partial J^{*}( \frac{\theta - p}{\lambda}) \\
  % 
  % 
  % 
  \Leftrightarrow 0
  &\in \frac{\theta - p}{\lambda} - \frac{\theta}{\lambda} + \frac{1}{\lambda} \partial J^{*}( \frac{\theta - p}{\lambda}).
\end{align*}

Denote $w: = \frac{\theta - p }{\lambda}$. We conclude that $w$
minimizes $\frac{1}{2}\| w - \frac{\theta}{\lambda}\|^2 +
\frac{1}{\lambda}J^{*}(w)$.  Since $J^{*}$ is the characteristic
function of $K$, we deduce $w = P_{K}( \frac{\theta}{\lambda} )$.
Recalling $p = \prox_{\lambda J}(\theta)$ and rearranging:

\begin{equation*}
  \prox_{\lambda J}(\theta) = \theta - P_{\lambda K}( \theta ).
\end{equation*}

Finding the projection amounts to finding the minimizer

\begin{equation*}
P_{\lambda K}(\theta) = \argmin_{|p_{ij}| - 1 \leq 0} \|\lambda \divg
p - \theta \|^2 = \argmin_{|p_{ij}| - 1 \leq 0} \| \divg p -
\frac{\theta}{\lambda} \|^2 / 2.
\end{equation*}

Now recall that $\nabla \|Ax - b \|^2/2 = A^*(Ax-b)$ and that $\divg =
-\grad^{*}$, by definition and the fact that the discrete gradient is
merely a linear operator. The Karush Kuhn Tucker conditions yield the
existence of a Lagrange multiplier $\mu_{ij}$ corresponding to every
inequality constraint $|p_{ij}| - 1 \leq 0$. For these and for a
minimum, it holds that $\forall i,j$:

\begin{align*}
  -\grad( \divg p - \frac{\theta}{\lambda} )_{ij} +  \mu_{ij} p_{ij} &= 0 \\
  %
  %
  %
  |p_{ij}|^2 - 1 & \leq 0 \\ 
  %
  %
  %
  \mu_{ij} &\geq 0 \\
  %
  %
  %
  \mu_{ij}( |p_{ij}|^2 - 1 ) &= 0.
\end{align*}

Thus, if $\mu_{ij} = 0$ then also $-\grad(  \divg p -\frac{\theta}{\lambda} )_{ij} = 0$.
If $\mu_{ij} > 0$ then $|p_{ij}| = 1$ and so $|\grad( \divg p - \frac{\theta}{\lambda} )_{ij}| = \mu_{ij}$.
Consequently,

\begin{equation*}
  |\grad(\divg p - \frac{\theta}{\lambda} )_{ij}| =  \mu_{ij}, \ \forall i,j.
\end{equation*}

Then a minimum will satisfy

\begin{equation*}
  \grad( \divg p - \frac{\theta}{\lambda} )_{ij} = |\grad( \divg p - \frac{\theta}{\lambda} )_{ij}|  p_{ij}.
\end{equation*}

Let $\tau > 0$. The following iteration is reasonable at least because
the minimum is a fixed point.
\begin{equation}
  p_{ij}^{n+1} = p_{ij}^{n} + \tau \left ( \grad (\divg p^{n} -
    \frac{\theta}{\lambda})_{ij} - | \grad (\divg p^{n} -
    \frac{\theta}{\lambda})_{ij}|p_{ij}^{n+1} \right ),
\end{equation}

which is equivalent to:

\begin{equation}
p_{ij}^{n+1} = \frac{p_{ij}^{n} + \tau \grad (\divg p^{n} - \frac{\theta}{\lambda})_{ij}}
{1+ \tau | \grad (\divg p^{n} - \frac{\theta}{\lambda})_{ij}|}.
\end{equation}

Chambolle \cite{chambolle2004algorithm} proves this converges for $0 <
\tau \leq \frac{1}{8}$ and states that $\tau = \frac{1}{4}$ is
optimal.

\subsection{Results for Chambolle's algorithm}
The proposed algorithm runs extremely fast, see resulsts in figure
\ref{cham}.

\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{test_cham.png}
\caption{Performance of Chambolle's algorithm. Noise amplitude is $\sigma = 15$. \label{cham}}
\end{figure}



\section{Calculating $\grad g$}
Lets do a calculation. In this section we define, for our kernel 
 $\bar{K}_{i-m,j-n} =  K_{m-i,n-j}, \ \forall m,n$.

\begin{align*}
  \langle Hu,v \rangle &= \langle K * u, v \rangle\\
  % 
  % 
  % 
  &= \sum_{ij} \sum_{mn} K_{i-m,j-n}u_{mn} v_{ij} \\
  % 
  %
  %
  &= \sum_{mn} u_{mn} \sum_{ij} K_{i-m,j-n} v_{ij} \\
  % 
  %
  %
  &= \sum_{ij} u_{ij} \sum_{mn} K_{m-i,n-j} v_{mn} \\
  % 
  %
  %
  &= \sum_{ij} u_{ij} \sum_{mn} \bar{K}_{i-m,j-n} v_{mn} \\
  % 
  %
  %
  &= \sum_{ij} u_{ij} (\bar{K} * v)_{ij} \\
  %
  % 
  %
  &= \langle u , \bar{K} *v \rangle \\
  %
  %
  %
  &= \langle u, H^{*} v \rangle,
\end{align*}

Specifically, if $m \equiv n \equiv 0 \mod N$ we have $\bar{K}_{i,j} =
K_{N-i,N-j}$. Recalling that $K_{ij} := \frac{1}{ (2m+1)^2 }
\mathbf{1}_{\{ 0 \leq i,j \leq 2m \} }$, we arrive at

\begin{align*}
  \bar{K}_{ij} &= \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ 0 \leq N-i,N-j \leq 2m \} } \\
  %
  %
  %
  &= \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ -N \leq -i,-j \leq 2m -N\} } \\
  %
  %
  %
  &= \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ N-2m \leq i,j \leq N \} } \\
\end{align*}

We may concolude that the gradient $\grad$ (wrt to each pixel, not
the discrete gradient) is:

\begin{align*}
  \grad \frac{1}{2\sigma^2} \|Hu - y\|^2 &= \frac{1}{\sigma^2} H^{*}(Hu -y ) \\
  &= \frac{1}{\sigma^2} \bar{K} * (K*u - y)\\
  &= \frac{1}{\sigma^2} K * (K*u - y),\\
\end{align*}
which can be very easily implemented using FFT. If we want to use
forward-backward, we must estimate the Lipschitz constant of the
gradient:

\begin{align*}
  \|\grad g(u) - \grad g(v)\| &= \|\frac{1}{\sigma^2} H^{*}(Hu - y ) - \frac{1}{\sigma^2}H^{*}(Hv - y)\| \\
  %
  %
  %
  &=\frac{1}{\sigma^2}\|H^{*}H\| \cdot \|u-v\|\\
  %
  %
  %
  &\leq \frac{1}{\sigma^2}\|H\|^2 \|u-v\| \\
  % 
  % 
  %  
  &= \frac{1}{\sigma^2} [\sup_{\|w\| = 1} \|Hw\|]^2 \|u-v\| \\
  %
  % 
  %
  &= \frac{1}{\sigma^2} [\sup_{\|\hat{w}\| = 1} \|\hat{K} \cdot \hat{w}\|]^2 \|u-v\| \text{ (FT) }\\
  %
  %
  %
  &= \frac{1}{\sigma^2} [\sup_{\|\hat{w}\| = 1} |\langle \hat{K},  \hat{w}\rangle|]^2 \|u-v\| \\
  %
  %
  &\leq \frac{1}{\sigma^2} \|\hat{K}\|^2 \|u-v\| \text{ (CS) }\\
  %
  %
  % 
  &\leq \frac{1}{\sigma^2} \|K\|^2 \|u-v\|\\
  %
  %
  %
  &=\frac{1}{\sigma^2} \frac{1}{(2m+1)^4} (2m+1)^4 \|u-v\| \\
  &= \frac{1}{\sigma^2} \|u-v\|.
\end{align*}

And so, in forward-backward terminology, $\beta = \sigma^2$. We will
thus take $\gamma :=\beta = \sigma^2$.  We see that $\delta := \min
\{1, \beta / \gamma \} + \frac{1}{2} = \frac{3}{2}$. Thus we may take
$\lambda_n \equiv 1$.



\section{Putting the pieces together}
I used Forward Backward to find $\theta^{(t+1)}$ in every step. Here
is the algorithm I implement. Recall that
\begin{align*}
  g(\theta)         &=  \frac{1}{2\sigma^2} \|H\theta - y\|_2^2, \\
  \gamma  h(\theta) &= \sigma^2 \alpha^{(t)} \|\grad \theta\|_2, \\
  \alpha^{(t)}       &= (N^2+1)( \|\grad\theta^{(t)} \|_2 + 1 )^{-1}. \\
\end{align*}

I took, as noted above, $\gamma = \sigma^2, \lambda_{n} \equiv 1$ in the forward backward algorithm. 
\begin{algorithm}
  \label{fwdbckwd}
  \caption{MM and FwdBckwd}
  \begin{algorithmic}[1]
    \State Set $\theta \gets y$ \Comment{The corrupted image.}    
    
    
    \For {$t = 1,2,3,...$} \Comment{MM cycles}

    \State $\alpha \gets (N^2+1)( \|\grad\theta \|_2 + 1 )^{-1}$.

    \For {$k = 1,2,3,...$} \Comment{Forward-Backward cycles}

   
    \State  $z \gets x - \frac{\gamma}{\sigma^2} K*(K*x - \theta)$ \Comment{$z \gets x - \gamma \nabla g( x )$}.

    \For {$n=1,2,3,...$} \Comment{Chambolle's cycles, calculating $\prox_{\gamma h }(z)$.}  

    \For {$i,j=1,...,N$}

    \State $z_{ij}^{tmp} \gets \frac{z_{ij} + \tau \grad (\divg
      z - \frac{\theta}{\lambda})_{ij}} {1+ \tau | \grad (\divg
      z - \frac{\theta}{\lambda})_{ij}|}$
    
    \EndFor
    
    \State $z \gets z^{tmp}$ 

    \EndFor

    \State $x \gets z$  \Comment{$x \gets \prox_{\gamma h}(z)$.}

    \EndFor

    \State $\theta \gets x$.

    \EndFor

    \State \Return $\theta$ \Comment{Deblurred and denoised image.}

  \end{algorithmic}
\end{algorithm}

\subsection{Final Results}
In figure \ref{fwdbckwd} below I show results for one forward backward
cycle with $\alpha$ set to be the ``true'' $\alpha$ (based on the
original image). Then, in figure \ref{all}, I show result for the
entire framework (including the majorisation-minimization steps). The
noise level is taken to be $\sigma = 0.05$. The Blurring kernel is
uniform $5 \times 5$.
 
\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{test_fwdbckwd.png}
\caption{Performance of one F-B cycle with the true $\alpha$. \label{fwdbckwd}}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{test_all.png}
\caption{Performance of the entire framework. \label{all}}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{refs.bib}

\end{document}















\section{Calculating $\prox_{g}$}
We need to calculate another proximity mapping. Recall that $H$ is a blurring operator defined by 
\begin{align*}
  [Hu]_{ij} &= \frac{1}{(2m+1)^2}\sum_{k=-m}^{m}\sum_{l=-m}^{m} x_{i+k,j+l} \\
  &= (u * K)(i,j), \\ 
\end{align*}
where $K_{ij} := \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ 0 \leq i,j \leq 2m \} }$. Denote the 2D DFT matrix by $F$. This is
a unitary operator. 
\begin{align*}
  \prox_{g}(\theta) &= \argmin_{u} \frac{1}{2\sigma^2} \|Hu - y\|^2 + \frac{1}{2} \|u-\theta\|^2 \\
  &= \argmin_{u} \frac{1}{2\sigma^2} \|(u * K) - y\|^2 + \frac{1}{2} \|u-\theta\|^2  \\
  &= \argmin_{u} \frac{1}{2\sigma^2} \|F(u * K) - Fy\|^2 + \frac{1}{2} \|Fu-F\theta\|^2 \\
  &= \argmin_{u} \frac{1}{2\sigma^2} \|\hat{u}\hat{K} - \hat{y}\|^2 + \frac{1}{2} \|\hat{u}-\hat{\theta}\|^2,
\end{align*}

where the multiplication in Fourier domain is pointwise. Now I find the minimum. For that, I seek to calculate the
zero of the gradient. For the sake of convenience, I drop the hats.

\begin{align*}
  0 &= \frac{\partial}{\partial u_{ij}} [\frac{1}{2\sigma^2}\| uK - y\|^2 + \frac{1}{2}\|\theta- u\|] \\
  &= \frac{\partial}{\partial u_{ij}} [\frac{1}{2\sigma^2} (u_{ij}K_{ij} - y_{ij})^2 + \frac{1}{2} (\theta_{ij} - u_{ij})^2] \\ 
  &= \frac{1}{\sigma^2} ( u_{ij}K_{ij} - y_{ij} ) K_{ij} + (u_{ij} -\theta_{ij} ) \\
  \Rightarrow u_{ij} &= (\frac{1}{\sigma^2}y_{ij}K_{ij} +\theta_{ij}) ( \frac{1}{\sigma^2}K_{ij}^2 + 1)^{-1} 
\end{align*}

So we conclude:

$$
\prox_{g}( \theta ) = F^{-1}\{(\frac{1}{\sigma^2}\hat{y}_{ij}\hat{K}_{ij} +\hat{\theta}_{ij}) ( \frac{1}{\sigma^2}\hat{K}_{ij}^2 + 1)^{-1} \},
$$

where all operations above are pointwise.

\subsection{Results}
In figure \ref{deblur} we demonstrate the performance of $\prox_{g}$.
We start with a clean image on top left. It is blurred by a $3 \times
3$ kernel. The result of $\prox_{g}$ with $\sigma = 10^{-4}$ gives,
essentially $\max_{\theta} \|H\theta - y\|^{2}$. Since no noise was
present, the original image is reconstructed perfectly.
\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{test_blur.png}
\caption{Performance of $\prox_{g}$. \label{deblur}}
\end{figure}
