\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{cite}
\usepackage{graphicx}
\usepackage{algorithm} % algorithm package
\usepackage[noend]{algpseudocode}
\usepackage[margin=0.5in]{geometry}
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DeclareMathOperator*{\argmin}{arg\,min}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text





%----------------------------------------------------------------------------------------
% new commands
%----------------------------------------------------------------------------------------
\newcommand{\data}{\mathbf{d}}
\newcommand{\param}{\theta}
\newcommand{\paramm}{\phi}
\newcommand{\coder}[1]{\texttt{#1}}
\newcommand{\inner}[2]{#1 \cdot #2}
\newcommand{\prox}{\text{Prox}}
\newcommand{\grad}{\nabla_{d} }
\newcommand{\modd}{\text{ mod }}
\newcommand{\divg}{\text{div}}
\newcommand{\fft}{\text{FFT}}
\newcommand{\ifft}{\text{i}\fft}

\title{Proximal Methods for Image Deblurring}

\author{Yair Daon}
\date{}

\pdfinfo{%
  /Title    ()
  /Author   (Yair Daon)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}


\begin{document}
\maketitle
\begin{abstract}
  I describe and implement a method of deblurring images using
  proximal operators.
\end{abstract}

\section{Problem Description}
I follow, somewhat loosely, a deblurring method presented in
\cite{green2015bayesian}. The goal is to recover a high resolution
image $\param \in \mathbb{R}^{N \times N}$ from a blurred and noisy
observation $\data \in \mathbb{R}^{N \times N}$. This obeserved image $\data$
is obtained from the original image $\param$ by a noisy convolution as
follows. Let $H: \mathbb{R}^{N \times N} \to \mathbb{R}^{N \times N}$
be the blurring operator. It acts as $H\param = \param * K$, where $K
\in \mathbb{R}^{N \times N}$ is a convolution kernel and the
convolution is taken with periodic boundary conditions.  The blurred
image is further corrupted by gaussian noise, which is $\mathcal{N}(0,
\sigma^2)$, additive and independent for every pixel. Thus $\data \sim
\mathcal{N}( H\param, \sigma^2I)$, where $I: \mathbb{R}^{N \times N}
\to \mathbb{R}^{N \times N}$ is the identity operator.

\section{Hierarchical Model and $\grad$}
This problem is ill posed and needs regularization. The regularization
method comes from the following bayesian hierarchical model suggested
by \cite{oliveira2009adaptive}.

\begin{align*}
  p(\data | \param) &= \frac{1}{ (2\pi \sigma^2)^{\frac{N^2}{2}}} \exp( -\frac{1}{2\sigma^2}\|H\param - \data \|_2^2 )
 \text{ (likelihood) }\\
  p( \param | \alpha ) &= \frac{1}{Z(\alpha)} \exp( -\alpha \|\grad\param\|_2 ) \text{ (prior) }\\
  p(\alpha) &= e^{-\alpha} \mathbf{1}_{\mathbb{R}_{+}}(\alpha) \text{ (hyperprior)}. \\
\end{align*} 

$\grad \param$ is the discrete gradient and is defined below with a
periodic boundary (slightly differently from the definition in
\cite{chambolle2004algorithm}):
\begin{align*}
  (\grad \param)_{ij} :&= \big ( (\grad \param)_{ij}^1 , ( \grad
  \param)_{ij}^2 \big ) \\
  %
  %
  (\grad \param)_{ij}^1 :&= \param_{i+1\modd N,j} -\param_{ij} \\
  %
  %
  (\grad\param)_{ij}^2 :&= \param_{i,j+1\modd N}
  -\param_{ij}.
  %
  %
\end{align*}

In the discussion below, I drop the $\mod$ term when referring to
$\grad$. In \cite{green2015bayesian}, the norm on the gradient in the
prior is taken to be ``the $l_1-l_2$ composite norm''. Here I use the
2-norm for the gradient, which is the one used in
\cite{chambolle2004algorithm, oliveira2009adaptive}.

\subsection{The posterior}
We seek the MAP estimator of $p( \param | \data )$. First we calculate
(estimate) the posterior $p( \param | \data )$. The following
discussion follows the one in \cite[section 4.1]{oliveira2009adaptive}
with the simplification that, in their notation, we take $\alpha =
\beta = 1$. First, we find the marginal $p( \param )$.

\begin{align*}
  p (\param) &= \int_{0}^{\infty}  p(\param| \alpha ) p(\alpha) d\alpha\\
  &= \int_{0}^{\infty}  p(\param| \alpha ) e^{-\alpha} d\alpha \\
  &= \int_{0}^{\infty}  \frac{1}{Z(\alpha)} \exp( -\alpha \|\grad\param\|_2 ) e^{-\alpha} d\alpha \\
\end{align*}

We calculate the normalization constant. Explanations follow.

\begin{align*}
  Z(\alpha) &= \int_{\mathbb{R}^{N \times N}} \exp( -\alpha \|\grad\param\|_2 ) d\param  \\
  &= \int_{\mathbb{R}^{N \times N}} \exp( -\alpha \sum_{i,j=1}^{N-1} \sqrt{ (\param_{i+1,j} -\param_{ij})^2 + (\param_{i,j+1} -\param_{ij})^2} ) d\param \\
  &\approx \left (\int_{\mathbb{R}^2}  \exp( -\alpha \sqrt{u^2 + v^2} )dudv \right )^{N^2} \\
  &= \left ( 2\pi \int_0^{\infty} \exp(-\alpha r) rdr \right )^{N^2} \\
  &= \left (2\pi \left (-\frac{r}{\alpha}\exp(-\alpha r) \big
  |_{r=0}^{r=\infty} +\frac{1}{\alpha} \int_0^{\infty}\exp(-\alpha
  r) dr \right ) \right)^{N^2}\\
  &= \left (\frac{2\pi}{\alpha^2} \right )^{N^2} \\
  & = C \alpha^{-\eta N^2}.
\end{align*} 

The first approximation builds on an assumption from
\cite{oliveira2009adaptive} that the graditents of different pixels
are independent. Then integral is calculated via integration by
parts. In the last line, $C$ is an irrelevant constant and $\eta =
2$. Since independence does not hold, \cite{oliveira2009adaptive} use
different values of $\eta$ for better performance ($\eta= 0.4$,
specifically) and \cite{green2015bayesian} uses $\eta = 1$ and this is
the value used here. Then:

\begin{align*}
  p (\param) &\approx \frac{1}{C} \int_{0}^{\infty} \alpha^{N^2 } \exp( -\alpha (\|\grad\param\|_2 + 1)) d\alpha \\
  %
  %
  %% &= \frac{1}{C} \int_{0}^{\infty} \alpha^{N^2} \exp( -\alpha (\|\grad\param\|_2 + 1)) d\alpha \\
  %
  %
  &= \frac{1}{C} \left (
  \frac{-\alpha^{N^2} \exp(-\alpha (\|\grad\param\|_2 + 1))}{||\grad\param||_2 + 1} \bigg |_{\alpha=0}^{\alpha=\infty}
  +\frac{N^2}{\|\grad\param\|_2 + 1}\int_{0}^{\infty} \alpha^{N^2-1} \exp( -\alpha (\|\grad\param\|_2 + 1)) d\alpha \right ) \\
  %
  %
  &= \frac{N^2}{C\|\grad\param\|_2 + 1}\int_{0}^{\infty} \alpha^{N^2-1} \exp( -\alpha (\|\grad\param\|_2 + 1)) d\alpha \\
  %
  &= \frac{ N^2!}{ C(\|\grad\param\|_1 + 1)^{N^2+1}} \text{ (repeated integration by parts)}\\
  &\propto (\|\grad\param\|_2 + 1)^{-N^2-1},
\end{align*}

which is equation (24) from \cite{oliveira2009adaptive} (with $\alpha
= \beta = 1$ by their notation). Putting the pieces together, the
posterior is:

\begin{align*}
  p(\param | \data ) &\propto p( \data | \param ) p(\param ) \\
  &\propto \exp \left ( -\frac{1}{2\sigma^2} \|H\param - \data\|_2^2 - (N^2 + 1) \log (\|\grad\param\|_2 + 1) \right ) \\
\end{align*}
 
and so we seek

\begin{equation}\label{eq:problem}
\param_{\text{MAP}} = \argmin_{\param} \frac{1}{2\sigma^2} \|H\param - \data \|_2^2 + (N^2 + 1) \log (\|\grad\param\|_2 + 1),
\end{equation}

which is the maximization problem (21) from \cite{green2015bayesian}
except that we use the 2-norm and not the composite 1,2-norm and our
$N^2$ is denoted there by $n$.

\section{Majorization - Minimization}\label{subsec:MM}

Problem \eqref{eq:problem} is not convex --- for once, log is concave.
This is circumvented in \cite{oliveira2009adaptive} by taking a
sequence of convex majorants.  Consider the problem of finding
$\hat{\param} \in \argmin_{x} L(\param)$, for some $L$. Carrying out
the majorization-minimization approach consists of finding a bound
$Q(\param; \param') \geq L(\param), \forall \param,\param'$ with
equality for $\param=\param'$ and then iterating $\param^{(t+1)} :=
\argmin_{\param} Q(\param;\param^{(t)})$. This iteration is monotone:
\begin{align*}
  L(\param^{(t+1)}) &= L(\param^{(t+1)}) - Q(\param^{(t+1)}; \param^{(t)}) + Q(\param^{(t+1)}; \param^{(t)})\\
  %
  %
  %
  &\leq  Q(\param^{(t+1)}; \param^{(t)}) \text{ by } Q \geq L \\
  %
  % 
  %
  & \leq Q(\param^{(t)} ;\param^{(t)}) \text{ by definition of } \param^{(t+1)} \\
  %
  %
  %
  &= L(\param^{(t)}) \text{ by the equality condition above. }
\end{align*}
Define 

$$
L(\param) :=\frac{1}{2\sigma^2} \|H\param - \data\|_2^2 + (N^2 + 1) \log (\|\grad\param\|_2 + 1).
$$

Note that $\forall z,z_0 > 0$:

$$
\log z \leq \log z_0 + \frac{z-z_0}{z_0},
$$ 
with equality iff $z = z_0$.

Use this inequality with $z = \|\grad\param\|_2 + 1, z_0 = \|\grad
\param^{(t)}\|_2 + 1$ to observe

\begin{align*}
  \log (\|\grad\param\|_2 + 1) &\leq \log( \|\grad \param^{(t)}\|_2 + 1 ) + 
  \frac{ \|\grad \param\|_2 + 1 - (\|\grad \param^{(t)}\|_2 + 1)}{\|\grad \param^{(t)}\|_2 + 1} \\
  &= C(\param^{(t)}) + \frac{ \|\grad \param\|_2 }{\|\grad \param^{(t)}\|_2 + 1}.
\end{align*}

Denote $\alpha^{(t)} := (N^2+1)( \|\grad\param^{(t)} \|_2 + 1 )^{-1}$. Then

\begin{align*}
  L(\param) &= \frac{1}{2\sigma^2} \|H\param - \data\|_2^2 + (N^2 + 1) \log (\|\grad\param\|_2 + 1) \\ 
  &\leq \frac{1}{2\sigma^2} \|H\param - \data\|_2^2 + \alpha^{(t)} \|\grad \param\|_2   + C(\param^{(t)})\\
  &=: Q(\param ; \param^{(t)}).
\end{align*}

Thus, we find the next approximation by:

\begin{align*}
  \param^{(t+1)} &:= \argmin_{\param} Q(\param, \param^{(t)}) \\
  &= \argmin_{\param}  \frac{1}{2\sigma^2} \|H\param - \data \|_2^2 + \alpha^{(t)} \|\grad \param\|_2
\end{align*}

and $C(\param^{(t)})$ is omitted since it does not affect the
minimizer. This is the problem that (should be) denoted by (22) in
\cite{green2015bayesian} (one of the authors made a typo and uses a
different $\alpha^{(t)}$. I confirmed this with the author). We
outline the method in Algorithm \ref{alg:MM} below. I will use the
forward backward algorithm \cite{combettes2011splitting} to minimize
$Q$ and it is outlined in the next section.

\begin{algorithm}
  \caption{Majorization Minimization}\label{alg:MM}
  \begin{algorithmic}
    \Procedure{MM}{\data}
    \State $\param \gets \data$
    \For {t=1,2,3,...}
    \State $\alpha \gets \frac{N^2 + 1}{\|\grad \param\|_2 + 1}$ 
    \State $\param \gets \argmin_{\paramm} \frac{1}{2\sigma^2} \|H\paramm - \data \|_2^2 + \alpha \|\grad \paramm\|_2$
    \EndFor
    \State \Return $\param$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\section{The forward backward algorithm}
I used a simplified version of the Forward-Backward algorithm
\cite[algorithm 3.4]{combettes2011proximal}. Let me outline the
notation and algorithm used there. We are given $f_1$ convex lower
semi continuous and $f_2$ convex and differentiable with Lipschitz
constant $\beta$. We seek $\argmin f_1 + f_2$ and have access to
$\nabla f_2$ and $\prox_{\beta^{-1} f_1}$. Note that I omit the
parameter $\lambda_n$ from said algorithm and take $\lambda_n \equiv 1$
for simplicity of implementation and exposition (read: laziness).

\begin{algorithm}
  \caption{Forward Backward}\label{alg:forward backward}
  \begin{algorithmic}
    \State Fix an arbitrary $x$.
    \For {$n = 1,2,3,...$}
    \State $y \gets x - \beta^{-1}\nabla f_2(x)$
    %% \State $\lambda_n \in [\epsilon, 3/2 - \epsilon]$
    %% \State $x_n \gets x_n +\lambda_n ( \prox_{\beta^{-1}f_1} y_n  - x_n)$
    \State $x \gets \prox_{\beta^{-1}f_1} y$
    \EndFor
    \State \Return $x$
  \end{algorithmic}
\end{algorithm}

We will use Algorithm \ref{alg:forward backward} in the minimization
step of Algorithm \ref{alg:MM}. In order to do so, we calculate
proximal maps and gradients in the following sections.

\section{Chambolle's Algorithm}
In this section I follow \cite{chambolle2004algorithm}. Let $J(\param) :=
\|\grad \param \|_2 = \sum_{i,j} |(\grad \param_{ij})|$.  Note that $J(\lambda
\param) = \lambda J(\param)$ for $\lambda \geq 0$ and also $J \geq 0$. If
$\exists \param_0$ s.t.  $\langle \paramm, \param_0 \rangle - J(\param_0) > 0$, then
$\langle \lambda \param_0, \paramm \rangle - J(\lambda \param_0) \to \infty$ as
$\lambda \to \infty$. Thus we may easily conclude,

\begin{align*}
  J^{*}(\paramm) :&= \sup_{\param} \langle \paramm, \param \rangle - J(\param) \\
  %
  %
  % 
  &= \sup_{\param} \sum_{i,j = 1}^{N} \paramm_{ij} \param_{ij} - J( \param ) \\ 
  %
  %
  %
  &= 
  \begin{cases}
    0 & \paramm \in K\\
    \infty & \paramm \not \in K.\\ 
  \end{cases}
\end{align*}
 
$K$ is convex since $J^{*}$ is. Since $J$ is convex lsc, we observe
that

\begin{align*}
  J(\param) &= J^{**}(\param) \\ 
  &= \sup_{\paramm} \langle \paramm, \param \rangle - J^{*}(\paramm) \\
  %
  %
  %
  &= \sup_{\paramm\in K} \langle \paramm, \param \rangle
\end{align*}

By Cauchy Schwarz (and its equality condition)   
\begin{align*}
  J(\param) &= \sum_{ij} | \grad \param | \\ 
  &= \sup_{|p_{ij}| \leq 1} \sum_{ij} (\grad \param)^1_{ij} p^{1}_{ij} + (\grad \param)^2_{ij} p^{2}_{ij} \\
  %
  %
  %
  &= \sup_{|p_{ij}| \leq 1} \langle \grad \param, p  \rangle \\
  %
  %
  &= \sup_{|p_{ij}| \leq 1} \langle \param, \grad^{*} p  \rangle \\
\end{align*}

with the obvious definition of an inner product. If we denote $ \divg
:= -\grad^{*}$, the negative adjoint of the discrete gradient
operator, then we may easily observe $K = \{ \divg p: |p_{ij}| \leq 1
\ \forall 1 \leq i,j, \leq N \}$. I won't write the expression for
$\divg$ here but it is extremely simple because of the periodic
boundary. We may now turn to deriving an algorithm for the proximity
mapping.

\begin{align*}
  \paramm :&= \prox_{\lambda J}\param \\
  %
  %
  %
  &= \argmin_{\paramm} \frac{1}{2}\| \paramm - \param \|^2 + \lambda J(\paramm) \\
  % 
  % 
  % 
  \Leftrightarrow 0 &\in \frac{\paramm - \param}{\lambda} + \partial J(\paramm) \\
  % 
  % 
  % 
  \Leftrightarrow \frac{\param - \paramm}{\lambda} &\in \partial J(\paramm) \\
  % 
  % 
  % 
  \Leftrightarrow \paramm &\in \partial J^{*}\left ( \frac{\param - \paramm}{\lambda}\right ) \\
  % 
  % 
  % 
  \Leftrightarrow 0
  &\in \frac{\param - \paramm}{\lambda} - \frac{\param}{\lambda} + \frac{1}{\lambda} \partial J^{*}\left ( \frac{\param - \paramm}{\lambda} \right ).
\end{align*}

Denote $w: = \frac{\param - \paramm}{\lambda}$. We conclude that $w$
minimizes $\frac{1}{2}\| w - \frac{\param}{\lambda}\|^2 +
\frac{1}{\lambda}J^{*}(w)$.  Since $J^{*}$ is the characteristic
function of $K$, we deduce $\frac{\param-\paramm}{\lambda} = w = P_{K}( \frac{\param}{\lambda} )$.
Recalling $\paramm= \prox_{\lambda J}\param$ and rearranging:

\begin{equation*}%%\label{eq:chambolle prox}
  \prox_{\lambda J}\param = \param -  \lambda P_{K}\left ( \frac{\param}{\lambda} \right ).
\end{equation*}

So

\begin{equation}\label{eq:chambolle prox}
  \prox_{\lambda J}\param = \param - P_{\lambda K} ( \param ).
\end{equation}

%% Since

%% \begin{eqnarray*}
%%     \lambda P_K\left (\frac{\param}{\lambda} \right ) &= \lambda \argmin_{x \in K} \|x-\frac{\param}{\lambda}\| \\
%%     %
%%     %
%%     %
%%     &= \lambda \argmin_{x/\lambda \in K} \|\frac{x-\param}{\lambda} \| \\
%%     %
%%     %
%%     %
%%     &= \lambda \argmin_{x \in \lambda K} \| x - \param \| \\
%% \end{eqnarray*}

Finding the projection amounts to finding the minimizer

\begin{equation*}
  P_{\lambda K}(\param) = \argmin_{|p_{ij}| - 1 \leq 0} \| \lambda \divg
  p - \param \|^2 = \argmin_{|p_{ij}| - 1 \leq 0} \| \divg p -
  \frac{\param}{\lambda} \|^2.
\end{equation*}

Now recall that $\nabla \|Ax - b \|^2/2 = A^*(Ax-b)$ and that $\divg =
-\grad^{*}$, by definition and the fact that the discrete gradient is
merely a linear operator. The Karush Kuhn Tucker conditions yield the
existence of a Lagrange multiplier $\mu_{ij}$ corresponding to every
inequality constraint $|p_{ij}| - 1 \leq 0$. For these and for a
minimum, it holds that $\forall i,j$:

\begin{align*}
  -\grad \left ( \divg p - \frac{\param}{\lambda} \right )_{ij} +  \mu_{ij} p_{ij} &= 0 \\
  %
  %
  %
  |p_{ij}|^2 - 1 & \leq 0 \\ 
  %
  %
  %
  \mu_{ij} &\geq 0 \\
  %
  %
  %
  \mu_{ij}( |p_{ij}|^2 - 1 ) &= 0.
\end{align*}

Thus, if $\mu_{ij} = 0$ then also $-\grad(  \divg p -\frac{\param}{\lambda} )_{ij} = 0$.
If $\mu_{ij} > 0$ then $|p_{ij}| = 1$ and so $|\grad( \divg p - \frac{\param}{\lambda} )_{ij}| = \mu_{ij}$.
Consequently,

\begin{equation*}
 \left |\grad \left (\divg p - \frac{\param}{\lambda} \right )_{ij} \right | =  \mu_{ij}, \ \forall i,j.
\end{equation*}

Then a minimum will satisfy

\begin{equation*}
  \grad \left ( \divg p - \frac{\param}{\lambda} \right )_{ij} = \left
  |\grad \left ( \divg p - \frac{\param}{\lambda} \right )_{ij} \right
  | p_{ij}.
\end{equation*}

Let $\tau > 0$. The following iteration is reasonable at least because
the minimum is a fixed point.
\begin{equation}
  p_{ij}^{n+1} = p_{ij}^{n} + \tau \left ( \grad \left (\divg p^{n} -
    \frac{\param}{\lambda} \right )_{ij} - \left | \grad \left (\divg p^{n} -
    \frac{\param}{\lambda} \right )_{ij} \right | p_{ij}^{n+1} \right ),
\end{equation}

which is equivalent to:

\begin{equation}
p_{ij}^{n+1} = \frac{p_{ij}^{n} + \tau \grad \left (\divg p^{n} - \frac{\param}{\lambda} \right )_{ij}}
{1+ \tau \left | \grad \left ( \divg p^{n} - \frac{\param}{\lambda}\right )_{ij}  \right | }.
\end{equation}

Chambolle \cite[Theorem 3.1]{chambolle2004algorithm} proves $\lim_{n \to \infty} \lambda
\divg p^n = P_{\lambda K}(\param)$ for $0 < \tau \leq \frac{1}{8}$ and
states that $\tau = \frac{1}{4}$ is optimal.

Hence, calculating $\prox_{\lambda J}\param$ is straightforward:

\begin{algorithm}
  \caption{Chambolle's Algorithm for $\prox_{\lambda J}\param$}\label{alg:chambolle}
  \begin{algorithmic}
    \Procedure{Chambolle}{$\param, \lambda$}
    \State $p \gets \param$
    \For {$n = 1,2,3,...$}

    \State $\forall i,j:\ \hat{p}_{ij} \gets \frac{p_{ij} + \tau \grad (\divg p - \frac{\param}{\lambda})_{ij}}
    {1+ \tau | \grad (\divg p - \frac{\param}{\lambda})_{ij}|}$
    \State $p \gets \hat{p}$
    \EndFor
    \Return $\param - \lambda \divg p$
  \end{algorithmic}
\end{algorithm}


%% \subsection{Results for Chambolle's algorithm}
%% The proposed algorithm runs extremely fast, see resulsts in figure
%% \ref{cham}.

%% \begin{figure}[ht!]
%% \centering
%% \includegraphics[width=150mm]{test_cham.png}
%% \caption{Performance of Chambolle's algorithm. Noise amplitude is $\sigma = 15$. \label{cham}}
%% \end{figure}



\section{The Gradient}
Lets do a calculation. In this section we define, for our kernel
$\bar{K}_{i-m,j-n} = K_{m-i,n-j}, \ \forall m,n$.

\begin{align*}
  \langle H\param,\paramm \rangle &= \langle K * \param, \paramm \rangle\\
  % 
  % 
  % 
  &= \sum_{ij} \sum_{mn} K_{i-m,j-n}\param_{mn} \paramm_{ij} \\
  % 
  %
  %
  &= \sum_{mn} \param_{mn} \sum_{ij} K_{i-m,j-n} \paramm_{ij} \\
  % 
  %
  %
  &= \sum_{ij} \param_{ij} \sum_{mn} K_{m-i,n-j} \paramm_{mn} \\
  % 
  %
  %
  &= \sum_{ij} \param_{ij} \sum_{mn} \bar{K}_{i-m,j-n} \paramm_{mn} \\
  % 
  %
  %
  &= \sum_{ij} \param_{ij} (\bar{K} * \paramm)_{ij} \\
  %
  % 
  %
  &= \langle \param , \bar{K} *\paramm \rangle \\
  %
  %
  %
  &= \langle \param, H^{*} \paramm \rangle,
\end{align*}

Specifically, if $m \equiv n \equiv 0 \mod N$ we have $\bar{K}_{i,j} =
K_{N-i,N-j}$. Recalling that $K_{ij} := \frac{1}{ (2m+1)^2 }
\mathbf{1}_{\{ 0 \leq i,j \leq 2m \} }$, we arrive at

\begin{align*}
  \bar{K}_{ij} &= \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ 0 \leq N-i,N-j \leq 2m \} } \\
  %
  %
  %
  &= \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ -N \leq -i,-j \leq 2m -N\} } \\
  %
  %
  %
  &= \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ N-2m \leq i,j \leq N \} } \\
\end{align*}

Let
\begin{equation*}
  g(\param) := \frac{1}{2\sigma^2} \|H\param - \data \|^2.
\end{equation*}
We may concolude that the gradient $\nabla$ (wrt to each pixel, not
the discrete gradient) is:

\begin{align*}
  \nabla g(\theta) &= \frac{1}{\sigma^2} H^{*}(H\param - \data ) \\
  &= \frac{1}{\sigma^2} \bar{K} * (K*\param - \data )\\
  &= \frac{1}{\sigma^2} K * (K*\param - \data),\\
\end{align*}
which can be very easily implemented using FFT.


\begin{algorithm}
  \caption{$\nabla g(\theta)$}
  \begin{algorithmic}
    \Procedure{Gradient}{$\param$}
    \State $\hat K \gets \fft(K)$
    \State $\hat \param \gets \fft\{ \param \}$
    \State $t \gets \ifft \{ \hat{K} \hat{\param }\}$
    \State \Return $\sigma^{-2}\ifft \{ \hat{K}(t - \data ) \}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


If we want to use forward-backward, we must estimate the Lipschitz
constant of the gradient:

\begin{align*}
  \|\nabla g(\param) - \nabla g(\paramm)\| &= \|\frac{1}{\sigma^2}
  H^{*}(H\param - \data ) - \frac{1}{\sigma^2}H^{*}(H\paramm -
  \data)\| \\
  %
  %
  %
  &=\frac{1}{\sigma^2}\|H^{*}H\| \cdot \|\param-\paramm\|\\
  %
  %
  %
  &\leq \frac{1}{\sigma^2}\|H\|^2 \|\param-\paramm\| \\
  % 
  % 
  %  
  &= \frac{1}{\sigma^2} \left ( \sup_{\|w\| = 1} \|Hw\| \right )^2 \|\param-\paramm\| \\
  %
  % 
  %
  &= \frac{1}{\sigma^2} \left ( \sup_{\|\hat{w}\| = 1} \|\hat{K} \cdot \hat{w}\| \right )^2 \|\param-\paramm\| \text{ (FT) }\\
  %
  %
  %
  &= \frac{1}{\sigma^2} \left ( \sup_{\|\hat{w}\| = 1} |\langle \hat{K},  \hat{w}\rangle| \right )^2 \|\param-\paramm\| \\
  %
  %
  &\leq \frac{1}{\sigma^2} \|\hat{K}\|^2 \|\param-\paramm\| \text{ (CS) }\\
  %
  %
  % 
  &\leq \frac{1}{\sigma^2} \|K\|^2 \|\param-\paramm\|\\
  %
  %
  %
  &=\frac{1}{\sigma^2} \frac{1}{(2m+1)^4} (2m+1)^4 \|\param-\paramm\| \\
  &= \frac{1}{\sigma^2} \|\param-\paramm\|.
\end{align*}

Thus, the Lipschitz constant of the gradient is $\sigma^{-2}$.

\section{Putting the pieces together}

Arranging all the pieces together we arrive at the following algorithm:
%% We denote
%% \begin{align*}
%%   g(\param) &= \frac{1}{2\sigma^2} \|H\param - \data\|_2^2, \\
%%   \nabla g(\param) & = \sigma^{-2} K * (K\param - \data), \\
%%   \text{Lip}\nabla g &= \sigma^{-2}, \\
%%   h(\param) &= \alpha^{(t)} \|\grad \param \|_2, \\
%%   \alpha^{(t)} &= (N^2+1)( \|\grad\param^{(t)} \|_2 + 1 )^{-1}.
%% \end{align*}


\begin{algorithm}
  \caption{Image deblurring and denoising}
  \begin{algorithmic}[1]
    \State Set $\param \gets \data$ \Comment{The corrupted image.}    
    \For {$t = 1,2,3,...$} \Comment{MM cycles}

    \State $\alpha \gets \frac{N^2+1}{\|\grad\param \|_2 + 1}$.

    \For {$n = 1,2,3,...$} \Comment{Forward-Backward cycles}
    \State $\paramm \gets \param - \sigma^2 \Call{Gradient}{\param}$ 
    \State $\param \gets \Call{Chambolle}{\paramm, \alpha\sigma^2}$ \Comment{Chambolle cycles are inside}
    \EndFor

    \EndFor
    \State \Return $\param$ \Comment{Deblurred and denoised image.}

  \end{algorithmic}
\end{algorithm}

\subsection{Final Results}
In figure \ref{fwdbckwd} below I show results for one forward backward
cycle with $\alpha$ set to be the ``true'' $\alpha$ (based on the
original image). Then, in figure \ref{all}, I show result for the
entire framework (including the majorisation-minimization steps). The
noise level is taken to be $\sigma = 0.05$. The Blurring kernel is
uniform $5 \times 5$.
 
\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{test_fwdbckwd.png}
\caption{Performance of one F-B cycle with the true $\alpha$. \label{fwdbckwd}}
\end{figure}


\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{test_all.png}
\caption{Performance of the entire framework. \label{all}}
\end{figure}

\bibliographystyle{unsrt}
\bibliography{refs.bib}

\end{document}















\section{Calculating $\prox_{g}$}
We need to calculate another proximity mapping. Recall that $H$ is a
blurring operator defined by
\begin{align*}
  [Hu]_{ij} &= \frac{1}{(2m+1)^2}\sum_{k=-m}^{m}\sum_{l=-m}^{m} x_{i+k,j+l} \\
  &= (u * K)(i,j), \\ 
\end{align*}
where $K_{ij} := \frac{1}{ (2m+1)^2 } \mathbf{1}_{\{ 0 \leq i,j \leq 2m \} }$. Denote the 2D DFT matrix by $F$. This is
a unitary operator. 
\begin{align*}
  \prox_{g}(\param) &= \argmin_{u} \frac{1}{2\sigma^2} \|Hu - y\|^2 + \frac{1}{2} \|u-\param\|^2 \\
  &= \argmin_{u} \frac{1}{2\sigma^2} \|(u * K) - y\|^2 + \frac{1}{2} \|u-\param\|^2  \\
  &= \argmin_{u} \frac{1}{2\sigma^2} \|F(u * K) - Fy\|^2 + \frac{1}{2} \|Fu-F\param\|^2 \\
  &= \argmin_{u} \frac{1}{2\sigma^2} \|\hat{u}\hat{K} - \hat{y}\|^2 + \frac{1}{2} \|\hat{u}-\hat{\param}\|^2,
\end{align*}

where the multiplication in Fourier domain is pointwise. Now I find the minimum. For that, I seek to calculate the
zero of the gradient. For the sake of convenience, I drop the hats.

\begin{align*}
  0 &= \frac{\partial}{\partial u_{ij}} [\frac{1}{2\sigma^2}\| uK - y\|^2 + \frac{1}{2}\|\param- u\|] \\
  &= \frac{\partial}{\partial u_{ij}} [\frac{1}{2\sigma^2} (u_{ij}K_{ij} - y_{ij})^2 + \frac{1}{2} (\param_{ij} - u_{ij})^2] \\ 
  &= \frac{1}{\sigma^2} ( u_{ij}K_{ij} - y_{ij} ) K_{ij} + (u_{ij} -\param_{ij} ) \\
  \Rightarrow u_{ij} &= (\frac{1}{\sigma^2}y_{ij}K_{ij} +\param_{ij}) ( \frac{1}{\sigma^2}K_{ij}^2 + 1)^{-1} 
\end{align*}

So we conclude:

\begin{equation*}
\prox_{g}( \param ) =
F^{-1}\{(\frac{1}{\sigma^2}\hat{y}_{ij}\hat{K}_{ij}
+\hat{\param}_{ij}) ( \frac{1}{\sigma^2}\hat{K}_{ij}^2 + 1)^{-1} \},
\end{equation*}

where all operations above are pointwise.

\subsection{Results}
In figure \ref{deblur} we demonstrate the performance of $\prox_{g}$.
We start with a clean image on top left. It is blurred by a $3 \times
3$ kernel. The result of $\prox_{g}$ with $\sigma = 10^{-4}$ gives,
essentially $\max_{\param} \|H\param - \data\|^{2}$. Since no noise
was present, the original image is reconstructed perfectly.
\begin{figure}[ht!]
\centering
\includegraphics[width=150mm]{test_blur.png}
\caption{Performance of $\prox_{g}$. \label{deblur}}
\end{figure}
